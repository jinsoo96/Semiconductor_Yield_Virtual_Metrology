{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semiconductor Yield Prediction - Modeling\n",
    "\n",
    "This notebook covers:\n",
    "1. Data Loading (from preprocessed data)\n",
    "2. Data Preparation (Train/Valid/Test split, Oversampling)\n",
    "3. AutoML with PyCaret\n",
    "4. Bayesian Optimization\n",
    "5. Model Evaluation & Prediction\n",
    "\n",
    "**Prerequisites:** Run `01_eda_and_preprocessing.ipynb` first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities\n",
    "from utils import *\n",
    "\n",
    "# Additional imports for modeling\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "try:\n",
    "    with open('../03_Results/preprocessed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train_raw = data['X_train']\n",
    "    y_train_raw = data['y_train']\n",
    "    X_predict = data['X_predict']\n",
    "    col_X = data['col_X']\n",
    "    col_selected = data['col_selected']\n",
    "    \n",
    "    print(\"Loaded preprocessed data successfully!\")\n",
    "    print(f\"X_train: {X_train_raw.shape}\")\n",
    "    print(f\"y_train: {y_train_raw.shape}\")\n",
    "    print(f\"X_predict: {X_predict.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Preprocessed data not found!\")\n",
    "    print(\"Please run 01_eda_and_preprocessing.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_raw, y_train_raw, \n",
    "    test_size=0.2, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Train/Validation split\n",
    "X_train_, X_valid, y_train_, y_valid = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_.shape}\")\n",
    "print(f\"Valid: {X_valid.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for oversampling\n",
    "os_df = X_train_.join(y_train_, how='left')\n",
    "\n",
    "# Create target categories for oversampling\n",
    "os_df.loc[os_df['y'] < 1242, 'y_cate'] = 'A'\n",
    "os_df.loc[(os_df['y'] >= 1242) & (os_df['y'] <= 1283), 'y_cate'] = 'B'\n",
    "os_df.loc[os_df['y'] > 1283, 'y_cate'] = 'C'\n",
    "\n",
    "print(\"Target distribution before oversampling:\")\n",
    "print(os_df['y_cate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform target (optional)\n",
    "os_df['log_y'] = np.log1p(os_df['y'])\n",
    "y_test_log = np.log1p(y_test)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "os_df_resampled, os_target = ros.fit_resample(\n",
    "    os_df.drop('y_cate', axis=1), \n",
    "    os_df['y_cate']\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter oversampling: {os_df_resampled.shape}\")\n",
    "print(\"Target distribution after oversampling:\")\n",
    "print(os_target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final training data\n",
    "feature_cols = [c for c in os_df_resampled.columns if c not in ['y', 'log_y']]\n",
    "X_train_final = os_df_resampled[feature_cols]\n",
    "y_train_final = os_df_resampled['log_y']  # Use log-transformed target\n",
    "\n",
    "print(f\"Final X_train: {X_train_final.shape}\")\n",
    "print(f\"Final y_train: {y_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AutoML with PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: PyCaret requires specific setup\n",
    "# Uncomment and run if pycaret is installed\n",
    "\n",
    "try:\n",
    "    from pycaret.regression import setup, compare_models, tune_model, evaluate_model, create_model, predict_model\n",
    "    PYCARET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyCaret not installed. Skipping AutoML section.\")\n",
    "    print(\"Install with: pip install pycaret\")\n",
    "    PYCARET_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYCARET_AVAILABLE:\n",
    "    # Prepare data for PyCaret\n",
    "    pycaret_df = os_df_resampled[feature_cols + ['log_y']].copy()\n",
    "    \n",
    "    # Setup PyCaret\n",
    "    reg = setup(\n",
    "        data=pycaret_df,\n",
    "        target='log_y',\n",
    "        normalize=True,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        session_id=123,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"PyCaret setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYCARET_AVAILABLE:\n",
    "    # Compare models\n",
    "    print(\"Comparing models (this may take several minutes)...\")\n",
    "    best = compare_models(sort='RMSE', n_select=5)\n",
    "    print(f\"\\nBest model: {type(best).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYCARET_AVAILABLE:\n",
    "    # Create individual models\n",
    "    print(\"Creating individual models...\")\n",
    "    \n",
    "    models = {}\n",
    "    model_names = ['et', 'rf', 'catboost', 'lightgbm', 'gbr']\n",
    "    \n",
    "    for name in model_names:\n",
    "        try:\n",
    "            models[name] = create_model(name, cross_validation=True, verbose=False)\n",
    "            print(f\"  - {name}: Created\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - {name}: Failed ({e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYCARET_AVAILABLE:\n",
    "    # Tune best model\n",
    "    print(\"Tuning best model...\")\n",
    "    best_tuned = tune_model(best, verbose=False)\n",
    "    print(\"Tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ridge Regression Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for Ridge\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_valid_scaled = scaler.transform(X_valid[feature_cols])\n",
    "X_test_scaled = scaler.transform(X_test[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ridge(alpha):\n",
    "    \"\"\"Objective function for Ridge optimization.\"\"\"\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train_final)\n",
    "    return model.score(X_train_scaled, y_train_final)\n",
    "\n",
    "# Bayesian Optimization for Ridge\n",
    "print(\"Optimizing Ridge Regression...\")\n",
    "ridge_optimizer = BayesianOptimization(\n",
    "    f=evaluate_ridge,\n",
    "    pbounds={'alpha': (0.01, 10)},\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "ridge_optimizer.maximize(init_points=5, n_iter=20)\n",
    "\n",
    "best_alpha = ridge_optimizer.max['params']['alpha']\n",
    "print(f\"\\nBest alpha: {best_alpha:.4f}\")\n",
    "print(f\"Best score: {ridge_optimizer.max['target']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Ridge model\n",
    "ridge_model = Ridge(alpha=best_alpha)\n",
    "ridge_model.fit(X_train_scaled, y_train_final)\n",
    "\n",
    "# Evaluate on validation set\n",
    "ridge_pred_valid = ridge_model.predict(X_valid_scaled)\n",
    "ridge_rmse_valid = rmse(y_valid_log, ridge_pred_valid)\n",
    "\n",
    "print(f\"Ridge Validation RMSE (log scale): {ridge_rmse_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_cv(n_estimators, min_samples_split, max_features, max_depth):\n",
    "    \"\"\"Objective function for Random Forest optimization.\"\"\"\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=int(n_estimators),\n",
    "        min_samples_split=int(min_samples_split),\n",
    "        max_features=min(max_features, 0.999),\n",
    "        max_depth=int(max_depth),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        model, X_train_final, y_train_final,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5\n",
    "    )\n",
    "    return np.nan_to_num(scores).mean()\n",
    "\n",
    "# Bayesian Optimization for Random Forest\n",
    "print(\"Optimizing Random Forest (this may take a while)...\")\n",
    "rf_optimizer = BayesianOptimization(\n",
    "    f=rf_cv,\n",
    "    pbounds={\n",
    "        'n_estimators': (50, 300),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'max_features': (0.1, 0.999),\n",
    "        'max_depth': (3, 15)\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_optimizer.maximize(init_points=5, n_iter=15)\n",
    "\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in rf_optimizer.max['params'].items():\n",
    "    print(f\"  {param}: {value:.4f}\")\n",
    "print(f\"Best CV score: {rf_optimizer.max['target']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest model\n",
    "best_rf_params = rf_optimizer.max['params']\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=int(best_rf_params['n_estimators']),\n",
    "    min_samples_split=int(best_rf_params['min_samples_split']),\n",
    "    max_features=min(best_rf_params['max_features'], 0.999),\n",
    "    max_depth=int(best_rf_params['max_depth']),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_pred_valid = rf_model.predict(X_valid[feature_cols])\n",
    "rf_rmse_valid = rmse(y_valid_log, rf_pred_valid)\n",
    "\n",
    "print(f\"Random Forest Validation RMSE (log scale): {rf_rmse_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train additional models for comparison\n",
    "models_comparison = {}\n",
    "\n",
    "# Extra Trees\n",
    "et_model = ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "et_model.fit(X_train_final, y_train_final)\n",
    "models_comparison['Extra Trees'] = et_model\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_final, y_train_final)\n",
    "models_comparison['Gradient Boosting'] = gb_model\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=200, random_state=42, verbose=-1)\n",
    "lgb_model.fit(X_train_final, y_train_final)\n",
    "models_comparison['LightGBM'] = lgb_model\n",
    "\n",
    "# Add our optimized models\n",
    "models_comparison['Ridge (Optimized)'] = ridge_model\n",
    "models_comparison['Random Forest (Optimized)'] = rf_model\n",
    "\n",
    "print(\"Models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON (Test Set)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models_comparison.items():\n",
    "    if name == 'Ridge (Optimized)':\n",
    "        pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        pred = model.predict(X_test[feature_cols])\n",
    "    \n",
    "    test_rmse = rmse(y_test_log, pred)\n",
    "    test_r2 = r2_score(y_test_log, pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': test_rmse,\n",
    "        'R2': round(test_r2, 4)\n",
    "    })\n",
    "    print(f\"{name:30s} | RMSE: {test_rmse:.4f} | R2: {test_r2:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('RMSE')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Best Model: {results_df.iloc[0]['Model']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model for final prediction\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models_comparison[best_model_name]\n",
    "\n",
    "print(f\"Using {best_model_name} for final predictions...\")\n",
    "\n",
    "# Generate predictions\n",
    "if 'Ridge' in best_model_name:\n",
    "    X_predict_scaled = scaler.transform(X_predict[feature_cols])\n",
    "    final_pred_log = best_model.predict(X_predict_scaled)\n",
    "else:\n",
    "    final_pred_log = best_model.predict(X_predict[feature_cols])\n",
    "\n",
    "# Convert back from log scale\n",
    "final_pred = np.expm1(final_pred_log)\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"Mean: {final_pred.mean():.2f}\")\n",
    "print(f\"Std: {final_pred.std():.2f}\")\n",
    "print(f\"Min: {final_pred.min():.2f}\")\n",
    "print(f\"Max: {final_pred.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'predicted_y': final_pred\n",
    "})\n",
    "\n",
    "predictions_df.to_csv('../03_Results/predictions.csv', index=False)\n",
    "print(\"Predictions saved to 03_Results/predictions.csv\")\n",
    "\n",
    "# Save best model\n",
    "with open('../03_Results/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Best model ({best_model_name}) saved to 03_Results/best_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(final_pred, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(final_pred.mean(), color='red', linestyle='--', label=f'Mean: {final_pred.mean():.2f}')\n",
    "axes[0].set_xlabel('Predicted Quality Index')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Prediction Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Actual vs Predicted (on test set)\n",
    "if 'Ridge' in best_model_name:\n",
    "    test_pred_log = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    test_pred_log = best_model.predict(X_test[feature_cols])\n",
    "test_pred = np.expm1(test_pred_log)\n",
    "y_test_actual = np.expm1(y_test_log)\n",
    "\n",
    "axes[1].scatter(y_test_actual, test_pred, alpha=0.5)\n",
    "axes[1].plot([y_test_actual.min(), y_test_actual.max()], \n",
    "             [y_test_actual.min(), y_test_actual.max()], 'r--', label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Quality Index')\n",
    "axes[1].set_ylabel('Predicted Quality Index')\n",
    "axes[1].set_title('Actual vs Predicted (Test Set)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../03_Results/figures/prediction_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to 03_Results/figures/prediction_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
